{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Mart Sales  - Prediction\n",
    "\n",
    "### The Problem Statement\n",
    "The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store.\n",
    "\n",
    "Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BigMartdata import getcsv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataframe :- (8523, 13)\n",
      "Test dataframe :- (5681, 12)\n",
      "Combined dataset dataframe :- (14204, 13)\n"
     ]
    }
   ],
   "source": [
    "dataset= getcsv_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BigMartdata import getcsv_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = getcsv_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8523, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.apply(lambda x: sum(x.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively use th following function\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter categorical variables\n",
    "categorical_columns = [x for x in dataset.dtypes.index if dataset.dtypes[x]=='object']\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively use th following function\n",
    "cat_cols = []\n",
    "for i in dataset.dtypes.index :\n",
    "    if dataset.dtypes[i]=='object' :\n",
    "        cat_cols.append(i)\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exclude Identifier columns and source column which was created newly:\n",
    "categorical_columns = [x for x in categorical_columns if x not in ['Item_Identifier','Outlet_Identifier','source']]\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print frequency of categories\n",
    "for col in categorical_columns:\n",
    "    print ('Frequency of Categories for varible : ', col)\n",
    "    print (dataset[col].value_counts())\n",
    "    print('---------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output gives us following observations:\n",
    "\n",
    "> Item_Fat_Content: Some of ‘Low Fat’ values mis-coded as ‘low fat’ and ‘LF’. Also, some of ‘Regular’ are mentioned as ‘regular’. <br>\n",
    "Item_Type: Not all categories have substantial numbers. It looks like combining them can give better results. <br>\n",
    "Outlet_Type: Supermarket Type2 and Type3 can be combined. But we should check if that’s a good idea before doing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Cleaning\n",
    "> Imputing missing values and Treating outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Item_Weight with the help of Item_Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.apply(lambda x: sum(x.isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the independet values, we can see that Item_Weight and Outlet_Size columns have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the average weight per item:\n",
    "Item_ave_weight = dataset.pivot_table(values='Item_Weight', index='Item_Identifier')\n",
    "Item_ave_weight.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a boolean variable specifying missing Item_Weight values\n",
    "miss_IW_rows = dataset['Item_Weight'].isnull() # Output is a list of boolean values\n",
    "miss_IW_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[miss_IW_rows,'Item_Weight'] = dataset.loc[miss_IW_rows,'Item_Identifier'].apply(lambda x: Item_ave_weight.loc[x])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Orignal #missing values:', sum(miss_IW_rows))\n",
    "print ('Final #missing values:', sum(dataset['Item_Weight'].isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Outlet_Size with the help of Outlet_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### a. Item_Weight with the help of Item_Identifier#Import mode function:\n",
    "from statistics import mode\n",
    "#Determining the mode for each\n",
    "Item_Outlet_size = dataset.pivot_table(values='Outlet_Size', index='Outlet_Type',aggfunc=(lambda x:mode(x)))\n",
    "Item_Outlet_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.Outlet_Type.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the mode parameter is not indicating the 'Grocery Store' value, <br> we will need to 'physically' impute the Outlet_Size as 'Small' to Outlet_Type 'Grocery Store'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_OS_GS_rows = dataset [(dataset['Outlet_Type'] == 'Grocery Store') & (dataset['Outlet_Size'].isnull())].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[miss_OS_GS_rows,'Outlet_Size'] = 'Small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_OS_rows = dataset['Outlet_Size'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[miss_OS_rows,'Outlet_Size'] = dataset.loc[miss_OS_rows,'Outlet_Type'].apply(lambda x: Item_Outlet_size.loc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Original missing : ', sum(miss_OS_rows))\n",
    "print ('After update missing : ' , sum(dataset['Outlet_Size'].isnull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering to __combine values__ in column __Outlet_Type__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " dataset.groupby(['Outlet_Type'])['Item_Outlet_Sales'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.pivot_table(values='Item_Outlet_Sales',index='Outlet_Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean outlet sales values *vary significantly* and hence we will leave them as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Item_Visibility with the help of Item_Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['Item_Visibility'] == 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Item_ave_visi = dataset.pivot_table(values='Item_Visibility', index='Item_Identifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Item_ave_visi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_val_ItemVis = (dataset['Item_Visibility'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[miss_val_ItemVis,'Item_Visibility'] = dataset.loc[miss_val_ItemVis,'Item_Identifier'].apply(lambda x: Item_ave_visi.loc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Number of 0 values in Item_Visibility initially:' , sum (miss_val_ItemVis))\n",
    "print ('Number of 0 values in Item_Visibility after impute:' , sum (dataset['Item_Visibility'] == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine another variable with means ratio\n",
    "dataset['Item_Visibility_MeanRatio'] = dataset.apply(lambda x: x['Item_Visibility']/Item_ave_visi.loc[x,'Item_Visibility'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a broad category of Type of Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first two characters of ID:\n",
    "dataset['Item_Type_Combined'] = dataset['Item_Identifier'].apply(lambda x: x[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Item_Type_Combined'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename them to more intuitive categories:\n",
    "dataset['Item_Type_Combined'] = dataset['Item_Type_Combined'].map(\n",
    "    {'FD':'Food','NC':'Non-Consumable','DR':'Drinks'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Item_Type_Combined'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the years of operation of a store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Years in operation :\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "dataset['Outlet_Since_Years'] = now.year - dataset['Outlet_Establishment_Year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modify categories of Item_Fat_Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change categories of low fat:\n",
    "print ('Original Item_Fat_Content Categories:' ) \n",
    "print (dataset['Item_Fat_Content'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Item_Fat_Content to proper names (consider upper case also)\n",
    "dataset['Item_Fat_Content'] = dataset['Item_Fat_Content'].replace({'LF':'Low Fat',\n",
    "                                                             'reg':'Regular',\n",
    "                                                             'low fat':'Low Fat'})\n",
    "print ('Item_Fat_Content Categories after renaming values :' ) \n",
    "print (dataset['Item_Fat_Content'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Marking non-consumables as separate category in Item_Fat_Content:\n",
    "dataset.loc[dataset['Item_Type_Combined']==\"Non-Consumable\",'Item_Fat_Content'] = \"Non-Edible\"\n",
    "dataset['Item_Fat_Content'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding of categorical variables to numericals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import library:\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New variable for outlet\n",
    "dataset['Outlet'] = le.fit_transform(dataset['Outlet_Identifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['Outlet_Identifier', 'Outlet']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols1 = []\n",
    "for i in dataset.dtypes.index :\n",
    "    if dataset.dtypes[i]=='object' :\n",
    "        cat_cols1.append(i)\n",
    "cat_cols1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding varaibles Outlet to the list and excluding identifiers and Item_Type\n",
    "var_mod = ['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Item_Type_Combined','Outlet_Type','Outlet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variables to numerics based on the above list (var_mod) created\n",
    "le_x = LabelEncoder()\n",
    "for i in var_mod:\n",
    "    dataset[i] = le_x.fit_transform(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Item_Type_Combined','Outlet_Type','Outlet']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot encoding of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One-Hot-Coding refers to creating dummy variables, one for each category of a categorical variable. \n",
    "\n",
    ">For example, the Item_Fat_Content has 3 categories – ‘Low Fat’, ‘Regular’ and ‘Non-Edible’. <br> \n",
    "One hot coding __will remove__ this variable __and generate__ 3 __new variables__. Each will have binary numbers – 0 (if the category is not present) and 1(if category is present). \n",
    "\n",
    ">This can be done using ‘get_dummies’ function of Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, columns=['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Outlet_Type',\n",
    "                              'Item_Type_Combined','Outlet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['Item_Fat_Content_0','Item_Fat_Content_1','Item_Fat_Content_2']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['Item_Type','Item_Type_Combined_0', 'Outlet_Establishment_Year', 'Outlet_Since_Years']].head()\n",
    "# Item type --> A new column 'Item_Type_Combined' was created mainly to obtain more counts in this categorical \n",
    "# varlaible, and was given intutive names based on the codes. The Item_Type column is 'repetition' of Item_Type_Combined.\n",
    "\n",
    "# Outlet_Establishment_Year --> A new column 'Outlet_Since_Years' was created using 'Outlet_Establishment_Year' column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns which have been converted to different types:\n",
    "dataset.drop(['Item_Type','Outlet_Establishment_Year'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset 'back' into test and train datasets:\n",
    "train = dataset.loc[dataset['source']==\"train\"]\n",
    "test = dataset.loc[dataset['source']==\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns:\n",
    "test = test.drop(['Item_Outlet_Sales','source'],axis=1) # dropping dependent variable 'Item_Outlet_Sales' from this dataset\n",
    "train = train.drop(['source'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting csv files as modified versions:\n",
    "train.to_csv(\"train_modified.csv\",index=False)\n",
    "test.to_csv(\"test_modified.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean based:\n",
    "mean_sales = train['Item_Outlet_Sales'].mean()\n",
    "mean_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a dataframe with IDs for submission:\n",
    "base1 = test[['Item_Identifier','Outlet_Identifier']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implying the mean sales value to the Item_Outlet_Sales column\n",
    "base1.loc[:,'Item_Outlet_Sales'] = mean_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the submission file\n",
    "base1.to_csv(\"alg0.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Function to automate submission to Hackothon__:\n",
    "> A generic function which takes the algorithm and data as input and makes the model, performs cross-validation and generates submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define target and ID columns:\n",
    "\n",
    "target = 'Item_Outlet_Sales'\n",
    "IDcol = ['Item_Identifier','Outlet_Identifier']\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "def modelfit(algorthm, dftrain, dftest, predictors, target, IDcol, filename):\n",
    "\n",
    "    #Fit the algorthmorithm on the data\n",
    "    # Creating ID columns and target columns in the new vriable\n",
    "    algorthm.fit(dftrain[predictors], dftrain[target]) # similar to the base dataframe created above with the predictor & target columns\n",
    "        \n",
    "    #Predict training set:\n",
    "    dftrain_predictions = algorthm.predict(dftrain[predictors]) # creating new predictor column \n",
    "\n",
    "    #Perform cross-validation:\n",
    "    cv_score = cross_val_score(algorthm, dftrain[predictors], dftrain[target], cv=20, scoring='neg_mean_squared_error')\n",
    "    cv_score = np.sqrt(np.abs(cv_score))\n",
    "    \n",
    "    #Print model report:\n",
    "    print (\"\\n------Model Report----\\n\")\n",
    "    print (\"RMSE : \" , np.sqrt(metrics.mean_squared_error(dftrain[target].values, dftrain_predictions)))\n",
    "    print (\"CV Score Mean : %.4g\" %(np.mean(cv_score)))\n",
    "    print (\"CV Score Std : %.4g\" %(np.std(cv_score)))\n",
    "    print (\"CV Score Min : %.4g\" %(np.min(cv_score)))\n",
    "    print (\"CV Score Max : %.4g\" %(np.max(cv_score)))\n",
    "    \n",
    "    #Predict on testing data:\n",
    "    dftest[target] = algorthm.predict(dftest[predictors])\n",
    "    \n",
    "    #Export submission file:\n",
    "    IDcol.append(target)\n",
    "    submission = pd.DataFrame({ x: dftest[x] for x in IDcol})\n",
    "    submission.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a list of independent varaibles to build the model\n",
    "target = 'Item_Outlet_Sales'\n",
    "IDcol = ['Item_Identifier','Outlet_Identifier']\n",
    "\n",
    "# Excluding Identifiers (used for submission) and dependent variable column\n",
    "predictors = [x for x in train.columns if x not in [target]+IDcol]\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a linear regression model \n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "alg1 = LinearRegression(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the FUNCTION which takes the algorithm and data as input and makes the model \n",
    "modelfit(alg1, train, test, predictors, target, IDcol, 'alg1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef1 = pd.Series(alg1.coef_, predictors).sort_values(ascending=True)\n",
    "coef1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "coef1.plot(kind='bar', title='Model Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a list of independent varaibles to build the model\n",
    "target = 'Item_Outlet_Sales'\n",
    "IDcol = ['Item_Identifier','Outlet_Identifier']\n",
    "predictors = [x for x in train.columns if x not in [target]+IDcol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Ridge regression model \n",
    "alg2 = Ridge(alpha=0.05,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelfit(alg2, train, test, predictors, target, IDcol, 'alg2.csv')\n",
    "coef2 = pd.Series(alg2.coef_, predictors).sort_values(ascending=True)\n",
    "plt.figure(figsize=(8,6))\n",
    "coef2.plot(kind='bar', title='Model Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a list of independent varaibles to build the model\n",
    "target = 'Item_Outlet_Sales'\n",
    "IDcol = ['Item_Identifier','Outlet_Identifier']\n",
    "predictors = [x for x in train.columns if x not in [target]+IDcol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Decision Tree regression model \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "alg3 = DecisionTreeRegressor(max_depth=15, min_samples_leaf=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(alg3, train, test, predictors, target, IDcol, 'alg3.csv')\n",
    "coef3 = pd.Series(alg3.feature_importances_, predictors).sort_values(ascending=False)\n",
    "plt.figure(figsize=(8,6))\n",
    "coef3.plot(kind='bar', title='Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that the RMSE is 1058 and the mean CV error is 1091. <br>\n",
    "This tells us that the __model is slightly overfitting__. <br> \n",
    "> Lets try making a decision tree with just top 4 variables, a max_depth of 8 and min_samples_leaf as 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a list of independent varaibles to build the model\n",
    "target = 'Item_Outlet_Sales'\n",
    "IDcol = ['Item_Identifier','Outlet_Identifier']\n",
    "# Making a decision tree with just **top 4** variables based on the above coeffecient values\n",
    "predictors = ['Item_MRP','Outlet_Type_0', 'Outlet_5', 'Outlet_Since_Years']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Decision Tree regression model \n",
    "# Considering a max_depth of 8 and min_samples_leaf as 150.\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "alg4 = DecisionTreeRegressor(max_depth=8, min_samples_leaf=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(alg4, train, test, predictors, target, IDcol, 'alg4.csv')\n",
    "coef4 = pd.Series(alg4.feature_importances_, predictors).sort_values(ascending=False)\n",
    "plt.figure(figsize=(8,6))\n",
    "coef4.plot(kind='bar', title='Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other parameters can be fine tuned to bring down the overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a list of independent varaibles to build the model\n",
    "target = 'Item_Outlet_Sales'\n",
    "IDcol = ['Item_Identifier','Outlet_Identifier']\n",
    "predictors = [x for x in train.columns if x not in [target]+IDcol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "alg5 = RandomForestRegressor(n_estimators=200,max_depth=5, min_samples_leaf=100,n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(alg5, train, test, predictors, target, IDcol, 'alg5.csv')\n",
    "coef5 = pd.Series(alg5.feature_importances_, predictors).sort_values(ascending=False)\n",
    "plt.figure(figsize=(8,6))\n",
    "coef5.plot(kind='bar', title='Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might feel this is a very small improvement but as our model gets better, achieving even minute improvements becomes exponentially difficult. <br>\n",
    ">Lets try another random forest with max_depth of 6 and 400 trees. Increasing the number of trees makes the model robust but is computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Random Forest model with max_depth of 6 and 400 trees\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "alg6 = RandomForestRegressor(n_estimators=400,max_depth=6, min_samples_leaf=100,n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(alg6, train, test, predictors, target, IDcol, 'alg6.csv')\n",
    "coef6 = pd.Series(alg6.feature_importances_, predictors).sort_values(ascending=False)\n",
    "plt.figure(figsize=(8,6))\n",
    "coef6.plot(kind='bar', title='Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__End Notes__<br>\n",
    "This article took us through the entire journey of solving a data science problem. We started with making some hypothesis about the data without looking at it. Then we moved on to data exploration where we found out some nuances in the data which required remediation. Next, we performed data cleaning and feature engineering, where we imputed missing values and solved other irregularities, made new features and also made the data model-friendly by one-hot-coding. Finally we made regression, decision tree and random forest model and got a glimpse of how to tune them for better results.\n",
    "\n",
    "I believe everyone reading this article should attain a good score in BigMart Sales now. For beginners, you should achieve at least a score of 1150 and for the ones already on the top, you can use some feature engineering tips from here to go further up. All the best to all!\n",
    "\n",
    "Did you find this article useful? Could you make some more interesting hypothesis? What other features did you create? Were you able to get a better score with GBM & XGBoost? Feel free to discuss your experiences in comments below or on the discussion portal and we’ll be more than happy to discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
